{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructured Data Analytics HW1\n",
    "\n",
    "Name:\n",
    "\n",
    "Andrew ID:\n",
    "\n",
    "Collaborators (if none, say \"none\"; do *not* leave this blank):\n",
    "\n",
    "Reminder: you should not be sharing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning: make sure your compute environment is set up first.** This homework assignment assumes that you have already installed Anaconda Python 3 and spaCy. Instructions for installing these are part of the tutorial [here](https://www.andrew.cmu.edu/user/georgech/95-865/Anaconda,%20Jupyter,%20and%20spaCy%20setup%20tutorial.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "1. Fill in your name, Andrew ID, and collaborators above.\n",
    "2. Fill in the code/text blocks to answer each question.\n",
    "3. Do *not* change any of the existing code provided.\n",
    "4. Run the entire notebook *before* submitting it on Canvas to make sure that the code actually runs without errors. (**Important**: Any code cells that you have entered code for but did not actually execute will be disregarded, so please be sure to actually run your code first and make sure it runs without errors! We may re-run a subset of your code for grading purposes.)\n",
    "5. Be careful about where you save data for use with this Jupyter notebook (more details on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Problem 1] Basic Text Analysis [45 pts]\n",
    "\n",
    "This problem involves a mix of Python review (loops, conditionals, counters/dictionaries, sorting) and learning to work with a sizable collection of text data.\n",
    "\n",
    "You will be looking at a phenomenon of \"natural languages\" (i.e., human languages, such as English) called *Zipf's law*, which relates how frequently a word occurs in a language to its \"rank\" (the word with rank 1 is the most frequently occurring word, the word with rank 2 is the second most frequently occurring word, etc). Roughly speaking, the word with rank 1 appears twice as likely as the word with rank 2, and the three times as likely as the word with rank 3, and so forth. (Some details on Zipf's law can be found on [Wikipedia](https://en.wikipedia.org/wiki/Zipf%27s_law).) In this problem, you are going to check whether Zipf's law holds for a real dataset of text documents.\n",
    "\n",
    "The dataset we look at is a collection of the 100 most popular books downloaded from the [Gutenburg Project](https://www.gutenberg.org/browse/scores/top). These 100 books form the corpus that we consider for this problem. Each file contains the text of a book. We will read in all 100 books.\n",
    "\n",
    "Note: Please *do not* change the folder name or the path, and make sure you use a relative path (e.g. './HW1_Dataset/*filename*) when reading the files. When grading your homework, we will put your Jupyter notebook file and the dataset in the same folder, and run your code. **You will not receive points for this problem if your code fails to load the data.**\n",
    "\n",
    "Hint: To list all files that match a certain pattern, you can use the `glob` package. Here's an example usage:\n",
    "\n",
    "```python\n",
    "import glob\n",
    "print(glob.glob('./HW1_Dataset/*.txt'))\n",
    "```\n",
    "\n",
    "**(a) Warm-up/basic Python review [15 pts across subparts].** This part serves as a warm-up, getting you familiar with the kind of code we will be writing in this class. Note that throughout part (a), your code should **not** be using spaCy.\n",
    "\n",
    "**Subpart i [5 pts].** Write a loop that iterates through all 100 books; for each book, print out its corresponding file name and also how long the book is in terms of string length (meaning that if we load in the book as a string, we compute the length of the string using the built-in Python function `len`; this is just counting the number of characters).\n",
    "\n",
    "**Please do not actually print out the contents of each book since many of the books are extremely long, and by printing out all the books' contents, you'll end up creating a Jupyter notebook that has a massive file size.**\n",
    "\n",
    "Hint: When debugging your code, you may want to first make sure your code runs on a few of the books rather than all 100 (for example, you can start by only having 3 of the text files in `HW1_Dataset`). Once you're confident that your solution is correct on a few text files, then run on all of them! This is a standard approach to debugging code that is meant to handle large datasets.\n",
    "\n",
    "Your output should look like (although there should be 100 books rather than 3 as shown below; also, the ordering of the books might be different on your machine):\n",
    "\n",
    "```\n",
    "./HW1_Dataset/War and Peace by graf Leo Tolstoy (251).txt 3227580\n",
    "./HW1_Dataset/Democracy in America â€” Volume 1 by Alexis de Tocqueville (147).txt 1148435\n",
    "./HW1_Dataset/Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley (501).txt 441034\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart ii [5 pts].** Copy and paste your code from **subpart i** into the code cell below. Then in the code cell below, modify the code so that it prints out the top 15 longest books (in terms of raw string length, which is what you had already computed). In particular, please write your code so that the printout is of the following format:\n",
    "\n",
    "```\n",
    "1. <number of characters in longest text file> <filename of longest text file>\n",
    "2. <number of characters in 2nd longest text file> <filename of 2nd longest text file>\n",
    "...\n",
    "15. <number of characters in 15th longest text file> <filename of 15th longest text file>\n",
    "```\n",
    "\n",
    "Note: only print out information for the top 15 books in the format above; please do not repeat printing what we asked you to print in **subpart i**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iii [5 pts].** You should find that some of the books are quite long. Processing very long books will be problematic with spaCy, so we will want to chop up long books into pieces. Note that later on in the course, we shall see that chopping up large amounts of data into small pieces or \"batches\" is in fact quite common in machine learning since for massive datasets, we often cannot store them in their entirety on a CPU or GPU for processing all at once.\n",
    "\n",
    "As a toy example, suppose that a book's text is `'cat dog shark spam eggs'` and we want to split it up into batches where each batch has at most 3 words. Then we could split up the book's text into two batches/pieces: `'cat dog shark'` and `'spam eggs'`.\n",
    "\n",
    "We have provided code for you to do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_into_batches(book_text, max_num_words_per_batch):\n",
    "    words_split_on_spaces = book_text.split(' ')\n",
    "    num_book_pieces = int(np.ceil(len(words_split_on_spaces) / max_num_words_per_batch))\n",
    "    batches = []\n",
    "    for piece_idx in range(num_book_pieces):\n",
    "        start_idx = piece_idx * max_num_words_per_batch\n",
    "        end_idx = (piece_idx + 1) * max_num_words_per_batch\n",
    "        if end_idx > len(words_split_on_spaces):\n",
    "            end_idx = len(words_split_on_spaces)\n",
    "        book_piece = ' '.join(words_split_on_spaces[start_idx:end_idx])\n",
    "        batches.append(book_piece)\n",
    "    return batches\n",
    "\n",
    "print(split_into_batches('cat dog shark spam eggs', 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a loop that goes through all 100 books again. However, for each book, split it up into batches so that each batch has at most 10000 words. In this case, how many batches are there total across all 100 books? Please write code that computes this number of batches. Your code should print out the following (with the correct total number of batches):\n",
    "\n",
    "```\n",
    "Total number of batches: <total number of batches across all 100 books>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [10 pts]** Now we will finally use spaCy. Note that for this problem, you don't actually need `spaCy`'s named entity recognition or grammatical parsing. Turning these elements off when you instantiate the `nlp` object can substantially speed up your code. To make sure these are off when instantiating the `nlp` object, call: \n",
    "\n",
    "```python\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "```\n",
    "\n",
    "**Throughout this problem, do not remove stopwords.**\n",
    "\n",
    "Build a term frequency (raw counts) table that is for all 100 books. Specifically, building on your solution to part (a)-subpart iii, read in a single book at a time and for each book, do the following:\n",
    "\n",
    "  1. Split it up into batches where each batch has at most 10000 words.\n",
    "  2. For each batch:\n",
    "    1. Process the batch by separating and lemmatizing the words\n",
    "    2. Count the number of times each lemma appears and add these to the frequency table. For simplicity, **do not convert lemmas to lowercase**. (Note that we use raw counts as the \"frequency\"--do not do any division.) Also, as we explain below, we will only count lemmas that are alphanumeric.\n",
    "\n",
    "Note that just as we had said in part (a): do **not** print out the complete contents of every book since doing so will result in a Jupyter notebook file that is massive.\n",
    "\n",
    "After looping through all 100 books, you should have the term frequency table for the entire corpus (importantly, the frequency table should not just be for a single book; it should be for all 100 books). Sort the table and print the top 50 most frequent words, along with their frequencies and ranks. Don't worry about ties (for example, if multiple things have the same frequency, it's fine if your solution breaks ties arbitrarily in the sorting).\n",
    "\n",
    "Note: When counting the lemmas, only include lemmas that consist of alphabetic letters (a-z and A-Z). You can do this with what's called a *regular expression*. For example, to check whether the words \"will.i.am\" or \"Tesla\" are alphabetic, you would do the following:\n",
    "\n",
    "```python\n",
    "import re  # regular expression package\n",
    "if re.match('[a-zA-Z]+$', 'will.i.am'):\n",
    "    print('will.i.am consists only of alphabetic letters!')\n",
    "if re.match('[a-zA-Z]+$', 'Tesla'):\n",
    "    print('tesla consists only of alphabetic letters!')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [10 pts]** Visualize the frequency table by plotting a **raw scatter plot** (put frequency as the y-axis and rank as the x-axis), and a **log-log plot** (use logarithmic scales on both the x- and y- axes). Note that this should be for all words and not only the top 50. As before, for the ranks, do not worry about ties, i.e., break ties arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw scatter plot\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-log plot\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) [10 pts across subparts]** Let's now try to make sense of the very last plot in part **(c)**. Zipf's law states that term frequency is governed by a power law, i.e. the relationship between term frequency and rank can be approximated by $f(r) = cr^{-1}$, where $f(r)$ is the frequency of the term at rank $r$, $r$ is the rank of a term, and $c$ is a constant that is approximately 0.1*(corpus size) for English.\n",
    "\n",
    "Please answer the following questions:\n",
    "\n",
    "**Subpart i [3 pts].** What do you observe in the log-log plot above? Is this consist with the power law?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your text answer (for this question, your answer is *not* code): *** WRITE YOUR ANSWER HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart ii [4 pts].** Think of the corpus as a (large) unigram bag of words. Following the analogy from lecture, imagine drawing a single word from this big bag (note that we are assuming that we've lemmatized the words and also filtered out non-alphanumeric words; thus what remains in the bag are actually alphanumeric lemmas). What is the probability of drawing one of the 4 most frequent alphanumeric lemmas? What is the probability of drawing one of the 50 most frequent alphanumeric lemmas? Answer these two questions using code rather than just entering in the final answers as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probability of drawing one of the 4 most frequent alphanumeric lemmas: ')\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################\n",
    "\n",
    "print('Probability of drawing one of the 50 most frequent alphanumeric lemmas: ')\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iii [4 pts].** What proportion of the alphanumeric lemmas occur only once? What proportion of the alphanumeric lemmas occur fewer than 10 times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Occur only once: \")\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################\n",
    "\n",
    "print(\"Occur fewer than 10 times: \")\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Problem 2] Entity Recognition and Pointwise Mutual Information (PMI) [50 pts]\n",
    "By using the entity recognition system in `spaCy`, let's identify named entities from newspaper articles. You'll be using Reuters corpus which contains more than ten thousand newspaper articles. To run the code below, you need to download the Reuters dataset. To do so, in a terminal/command line (recall that you can open a terminal from Jupyter's webpage that shows all the files, which by default is [http://localhost:8888/tree](http://localhost:8888/tree)), start up Python and enter:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "```\n",
    "\n",
    "Then proceed to the problem subparts below.\n",
    "\n",
    "Note that in this problem you will need named entity recognition but not grammatical parsing. Hence, you will want to instantiate the nlp object by calling:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) [15 pts]** Draw a bar chart in which one of the axes shows entity labels and the other shows the frequency of the corresponding label. Use the variables `reuters_nlp` and `label_counter` provided in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger'])\n",
    "reuters_fileids = reuters.fileids()  # hint: when first debugging, consider looking at just the first few\n",
    "reuters_nlp = [nlp(re.sub('\\s+',' ', reuters.raw(i)).strip()) for i in reuters_fileids]\n",
    "label_counter = Counter()\n",
    "\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [15 pts]** Now list the top 10 most frequently occurring entities (entity text and the number of occurence) with label `ORG` (organization). Separately list the top 10 most frequently occurring entities with label `GPE` (geopolitical entity such as countries, cities, states) respectively. **In both cases, please convert the entity names to lowercase first before computing the top 10.**\n",
    "\n",
    "Here, when counting the (raw count) frequency, we need to count how many articles have an entity with the desired property. For every article, we add 1 if the article has the entity and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [20 pts]** Give the top 50 `GPE` (countries, cities, states) entities that have the highest Pointwise Mutual Information (PMI) values with regard to the `ORG` (organization) entity **'opec'** (your list of this top 50 should be ranked in decreasing PMI value). Did you find any unexpected results? If so, why do you think it happened? If you found some of the results to be unsurprisingly, how come? **Just like in the previous part, please convert entity names to lowercase in your analysis.**\n",
    "\n",
    "Hint 1: As in lecture, when computing PMI, we will compute probabilities by counting the number of documents where entities occur or co-occur.  For example, $P('opec') = \\frac{number \\ \\ of \\ \\ documents \\ \\ containing \\ \\ 'opec'}{number \\ \\ of \\ \\ documents}$.  \n",
    "\n",
    "Hint 2: To compute this ranking, you do not have to compute the full PMI equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your text answer (for this question, your answer is *not* code): *** WRITE YOUR ANSWER HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Problem 3] Questionnaire [5 pts]\n",
    "\n",
    "Please provide feedback! To receive the 5 points, be sure to enter your Andrew ID correctly in this Google form: https://forms.gle/3XY8wdBDEKteN1xn9"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
